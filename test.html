                <meta charset="utf-8" emacsmode="-*- markdown -*-">
                            **2D Sound Localization System**
	**Kevin Chen, Julia Arnold, Kendall Yu, and Aaron Fleischer**

Overview
=========================================================================
The system is designed to perform the localization of a loud burst of sound within a room or area. With several connected ESPs around the room sending data on command sent via a connecting wire, a visit to our 6.08 development server shows the likely location of the most recent sound.

We imagine this to be useful in a day to day context, such as locating frustrated students during 6.08 office hours. On a larger scale, this is a proof of concept for larger real-life applications, including the following projects:
* Sound source separation, separating the source signals from a mixed signal with little to no information about the source signals or mixing process
* Sound source tracking, finding where a sound is coming from
* Sonar uses sound source localization techniques to identify the location of a target <sup> 1 </sup>
* Effective human-robot interaction, for human-machine interface<sup> 2 </sup>, handicapped aid, and military applications (i.e. the robot can turn to face a human that is talking) <sup> 1 </sup>

![Figure [day_in_lab]: So cute.](./images/day_in_lab.JPG width="400px" border="1")

System Documentation
=========================================================================
In this section we will explain our hardware, software, system functionality, and server operation.

<a href="http://www.youtube.com/watch?feature=player_embedded&v=tNd-7cINhSE
" target="_blank"><img src="http://img.youtube.com/vi/tNd-7cINhSE/0.jpg" 
alt="This video link shows our general hardware use and operation procedure." width="400" height="180" border="10" /></a>

Hardware
---

In terms of hardware, the setup is relatively simple.

Each ESP uses the following parts from our original kits:
* ESP32 microprocessor
* Battery, power cord and power management board
* Microphone

The other equipment necessary was roughly 100 meters of double wire cable to connect the ESPs together. The double wire was necessary in order to give a shared ground to reliably send a 3.3 V signal on the wire. Each ESP was then comparing to the same ground in order to recognize when the signal was up or down.

![Figure [system_setup]: This example sub ESP shows the minimal board setup with just the ESP, battery, and microphone. The signal wiring is also shown to pin 5 and ground.](./images/system_setup.png width="400px" border="1")

Our current implementation involves one master ESP (the ESP with ID 0) and six sub ESPs (with ID 1 through 6) spread out around the 6.08 lab space. To conserve power, the subs have only the ESP32, a microphone, and the battery board and battery. The master has an additional button wired to pin 17 to signal to the subs to start recording.

Each of these subs are wired to the master. In order to wire the subs, the ground (black) wire of the cable was connected to any ground of the ESP. The white wire is wired to pin 5 of each of the subs. In order to wire the master, the ground is also connected to ESP ground. The white wires are connected to pin 16 to signal on the rising edge of a 3.3 V signal when the button is released.

The microphone for each board has the OUT pin wired to the VP pin of the ESP, as well as a connection to 3.3 V and ground.

Software
---
The software is split between the ESPs and the server. The ESPs wait for signals from the master or the user, record sound, and transmit to the server. They wait for an acknowledgement from the server, and retry a limited number of times until they succeed. The server stores the recordings from the ESPs, returns displays for localized sounds, and provided additional utility for ease of development. sound.py performs the server functions of handling GETs and POSTs. A POST adds an additional row to sound.db and returns “ACK” to acknowledge receipt, while a GET response is html code showing the latest detected origination location of the sound signal in relation to the labelled ESPs. To calculate the location from database entries, it calls on get_location(). encode64() and decode64() convert between a list integers in the range 0 to 63 and a base64-encoded string created by converting each integer to its base64 encoding.
get_handler() retrieves the most recent data sent by each ESP, decodes them, calculates the relative times, and calculates the location. Sound_localization.py does the heavy lifting in computing the localization, and the exact method used is explained in a later section. Graphics.js contains the javascript code to update an html canvas, and graphics.py fills in an html template with a given sound location. The server also provides functionality in summary.py and data.py to return what and when the most recent recordings were and to return the entire recording for each device.

System Operation
---
For system setup, the first step is to upload the ESP code to each ESP, while making sure to update the ID for each individual ESP. For the initial setup, we measured the location of each ESP to the accuracy of .01 meters. Each ESP receives an assigned ID, which is related to a location in the lab. Each ID-location pair is recorded and used by the server each time the system is operated. In order to change the ID-location pairs, calibration.py and graphics.js should be updated.

The algorithm we chose needs at least 4 locations in order to find the position on the shared 2D plane. More are needed for a 3D system, which was beyond the scope of this project. We chose to have 7 ESPs total because sometime individual system failures due to spotty WiFi coverage, server timeouts, or not picking up the noise.

![Figure [lab_diagram]: The ESPs are placed throughout the 6.08 lab.](./images/lab_diagram.png width="300px" border="1")

When all ESPs are connected and powered on, the system is ready to go. On a high level, each ESP waits for either the button push if it is the master or the signal from the master if it is a sub. Upon the release of a button at pin 17, the master ESP outputs a 3.3 V signal on pin 16. This master pin is wired to each pin 5 of the subs. Directly after this signal, each ESP begins recording sound data from the microphone. The operators (us) make a loud sound within 4 seconds. After 4 seconds, the ESPs stop recording and send the sound data to the server. Upon a GET request, the server will return the calculated location of the sound using the sound localization algorithm. This process is illustrated in the block diagram below.

![Figure [block_diagram]: The block diagram illustrates the overall process connecting the master and sub ESPs as the signal is sent, all ESPs record data, and send it to the server.](./images/block_diagram.png width="400px" border="1")

ESP Function
---

The ESPs’ job is to connect to WiFi and wait. The master waits for a button push, and then it sends a 3.3 V signal to the subs. The subs wait for the signal from the master. Once the signal is given, all of the ESPs record sound for 4 seconds and then send the sound data to the server. After they send the data, they return to waiting. This process is illustrated in the state diagram shown below.

![Figure [ESP_state_diagram]: The ESP process is fairly simple in that the ESP sends or receives the signal, records 4 seconds of sound, and then sends the data to the server.](./images/ESP_state_diagram.png width="400px" border="1")

Server Function
---

The server has several functions. While the code is documented in detail above, the general operation of the server is to wait for a GET or POST request. Upon a POST request, the handler stores the ID, the data received, the exact length the ESP recorded for, and the time the server received the POST. This allows the server to account for ESP clock rate differences and to differentiate which recordings are relevant to the current sound.

Upon receiving a GET request, the server fetches the most recent timestamp for each ID. Then the beginning of each sound is found by using modified zero crossing. A naive approach is too noisy, so we required the data to pass certain high and low thresholds before it would be considered a crossing. The location of the original loud sound is determined by the highest density of crossings in a fixed size window. In the case of multiple candidates, the set of signals across all the devices which has the minimum maximum time between signals is chosen with a bias towards later signals in the event of ties. These detected signals are used to solve for the original signal location using the mathematical model explained in the next section. This location is nicely shown in the web display that visually shows the location and returns the exact calculated location in centimeters. The web display and server state diagram are shown below.

![Figure [server_state_diagram]: The server is responsible for receiving the data via POST, storing it, and then responding to a GET request to return the sound’s most recent location via a Canvas script.](./images/server_state_diagram.png width="400px" border="1")

![Figure [web_display]: This web display can be called from the server and shows where each ESP is located and where the most recent sound was likely located in centimeters.](./images/web_display.png width="300px" border="1")

Discussion and Rationale
=========================================================================

Algorithm for Calculating Sound Location
---
The first challenge was finding an algorithm for the localization, for which we chose to model each sound as a spherical wave. For simplicity, we will discuss the case in two dimensions. Extending this to three dimensions is straightforward. A wave is characterised by $(x_0,y_0,t_0)$ where $(x_0,y_0)$ is the origin of the sound at time $t_0$. At time $t$, the wave will have arrived at all locations $(x,y)$ such that $$(x-x_0)^2+(y-y_0)^2=v^2(t-t_0)^2$$ where $v$ is the speed of sound. By simple manipulation, $$x^2+y^2-v^2t^2=2xx_0+2yy_0-2v^2tt_0-(x_0^2+y_0^2-v^2t_0^2).$$ The detections of the sound are modeled by $(x_1,y_1,t_1),\cdots,(x_n,y_n,t_n)$ where microphone $i$ at $(x_i,y_i)$ first detects the sound at $t_i$. From this, we can compile the equation $$\begin{bmatrix} x_1^2+y_1^2-v^2t_1^2 \\ \vdots \\ x_n^2+y_n^2-v^2t_n^2 \end{bmatrix} = \begin{bmatrix} 2x_1 & 2y_1 & -2v^2t_1 & -1 \\ \vdots & \vdots & \vdots & \vdots \\ 2x_n & 2y_n & -2v^2t_n & -1 \end{bmatrix} \begin{bmatrix} x_0 \\ y_0 \\ t_0 \\ x_0^2+y_0^2-v^2t_0^2 \end{bmatrix}.$$ Those matrices can be easily constructed and solved for using Numpy. It appears to be reasonably stable to simulated error. While this method is decidedly non-ideal, it is sufficient for our uses. One unfortunate consequence of this method is that it requires 4 devices to converge to a unique solution whereas a more sophisticated method would only require 3.

As a rough estimate, in order to localize to within one meter, it would be necessary to detect the time at which the sound signal arrived at the device within hundreds of microseconds. This gave rise to two problems: syncing and signal detection.

Syncing the Timing of the ESPs and Choosing Calibration Method
---
Because each microphone is associated with a different ESP, it is not enough that each ESP be able to pinpoint the time at which it receives a signal, as each has a different clock. We tested several sync methods: GPS, sound, light, and wire (WiFi queries are much too slow).

Because WiFi is so slow, we also chose not to use the server to initiate the sync. Each ESP would not have received the signal at the same time and therefore would have started recording at different times. 

With the GPS module available, it was difficult to extract data with acceptable precision. The time data sent were not microsecond-precise, and they arrived at unpredictable times within the second rather than at a consistent edge.

We purchased [infrared LEDs](https://www.adafruit.com/product/388) and [receivers](https://www.adafruit.com/product/157) to test the possibility of sending and receiving infrared signals. The particular parts were tuned to a relatively specific frequency used by television remotes, but signals were sometimes missed. This effect would have been magnified with devices placed far away in a furnished room if a re-sync became necessary. The sound sync method ([using piezo buzzers](https://www.adafruit.com/product/160)) is similar, but it adds additional complexity in having to take into account the speed of sound travel and filtering the relevant sound from background noise.

A wired connection turned out to be the most reliable method with minimal latency, guaranteeing microsecond-level sync and consistent results. Even with a long (~11m) wire, the synced devices showed an imperceptibly increased error of 3 microseconds. In the following images, one with ESPs syncing through a short wire and the next with a long wire, one ESP sends data then receives data back, while the other does the opposite. The time at which the ESPs send data are chosen at random within a set interval. For the one that sends and then receives, the time difference between the time it starts sending the signal and the time it stops is displayed. For the other, the time difference is between the time it receives the signal and the time it starts sending it. The goal is that the microsecond differences displayed (under “Difference”) is the same for each interval of signal sending.

![Figure [short_wire_test]: We first conducted a short wire test to see if wire syncing was feasible.](./images/short_wire_test.jpg width="400px" border="1") 

![Figure [long_wire_test]: Following our selection of wire to sync, we conducted a test to measure how long it took for the signal to be sent through the wire.](./images/long_wire_test.jpg width="400px" border="1") 

While our current architecture is not very susceptible to the downfalls of clock drift, it is useful to know in case of a redesign. Currently, the ESPs all begin at the same time within 3 microseconds thanks to the wire signal. With two ESPs running different loops, the following shows the difference in their clock offsets over time. Since it is very close to linear, it would be possible to calculate and correct for the clock offsets. 

![Figure [clock_drift]: We measured and plotted clock drift over several minutes to be able to understand its potential effect on measurement accuracy.](./images/clock_drift.png width="400px" border="1") 

Method of Detecting of Loud Sound Despite Noise Interference
---
To distinguish our sound signal from background noise, we use zero crossing to find candidates which are filtered by choosing the closest set. This approach could still benefit from many additional filters such as eliminating out of date recordings from consideration, considering deviation from a smoothed signal, and the different sound characteristics between sound and our signal. In our current approach, we compute zero crossings as when the signal goes from above a high threshold to below a low threshold or visa versa. By using high and low thresholds, we can ignore false positive zero crossings created by random noise. From there, we choose the shortest interval of 50 zero crossings to be our signal. In the case of ties, we pick the set of signals which minimizes the maximum time gap between signals. This filters out brief swings in noise, but may have issues with accuracy and long signals.

There are still a few additions which would have been helpful for signal detection. First, only recordings which were recently submitted should be considered to avoid confusing different recordings. Second, signal candidates should receive a confidence score for better filtering. That way, recordings without a detected signal can be ignores. Third, a frequency based approach may be more successful because noise and ambient sound has a lower frequency than our signal. Fourth, other sounds have a sound signal which is much fuzzier than our signal. That characteristic could help distinguish the signal. These additions could help increase the accuracy of our signal detection which proved to be one of the limiting factors in our project.


Rate and Frequency to Record Sound Data
---
Due to memory constraints on the ESPs, we had to limit both the quality and the frequency of our recordings. Initially we wanted each ESP to be taking in sound constantly and performing cross-correlation in order to detect when it heard the sound we were looking for. That proved infeasible in a single-threaded environment. Eventually, we scaled down to record a certain amount of data, starting when we told the system there would be a sound to look for. This way, we could send the data to the server where it could be processed in Python instead of trying to calculate cross-correlation while the system is running.

Fixing the Location of ESPs
---
Due to the many variables already apparent in the system, we decided to fix the locations of the ESPs. With ESPs hardcoded with their ids and always placed at their marked positions, the locations of each device can be known with far better accuracy for sound localization and the wires connecting the ESPs, additionally, would stay safely at peripheral positions in the room. This also eliminated the need for a GPS, improving battery life.

Issues Addressed to Send Data to Server
---
Once all the sound is recorded on each ESP, it must be transmitted to the server to be saved for when localization occurs. The primary limitations are that the ESP is fundamentally single-threaded and that the ESPs have limited memory. Multithreading would have allowed for simultaneous sending and recording which would also be limited by transmitting speeds. Memory proved to be a larger problem because the entire recording must be stored in memory before it is sent. We could only store 50,000 samples in base64 encoding using 50kB. Since this data is encoded in base64, it only provides a resolution of 64 distinct values to the server. The limited memory imposes a tradeoff between recording quality, recording frequency, and recording length. This proved to be the limiting factor instead of the cycle time of taking a recording.

System Calibration
---
Even though all the ESPs are running the same code on the same components and all the subs are following the same path through that code, they often return different recording lengths due to inconsistent clock rates. To counteract that, we can record many signals from a known location. With that, we can compute the expected offsets between when the sound should be detected at each microphone. The difference between the expected offsets and the measured offsets can be used to construct an initial offset and a clock rate correction between each sub and the master. This can help reduce the inconsistencies between the recordings to an acceptable level.

Approach to Energy Management
---
In order to make our system last as long as possible, we designed it to need only the microphone. All other kit components were removed.

 These runtimes were calculated using the 1452 mWh capacity of the battery and battery management board as a unit. 

Next, we measured the current consumed in each state of a hypothetical cycle, assuming we send a signal every 5 minutes. The states are when the ESP is waiting for a signal (or button push), collecting sound data, and sending the data to the server. The stats for waiting and collecting are the same because we measured that the microphone is always consuming the same current. The calculations for power can be found in the table below.

| State                	| Time in state per 5 min | Current | Power 	| Power per Cycle (5 min) |
|--------------------------|-------------------------|---------|-----------|-------------------------|
|  Waiting for Signal or Collecting Sound Data | 4.9915 min          	| 17.2 mA | 56.76 mW  | 4.72 mWh            	|
| Sending Sound Data   	| 509 ms              	| 86.2 mA | 284.46 mW | 0.04 mWh            	|
| Total                	| 5 min               	| N/A 	| N/A   	| 4.76 mWh            	|

Therefore, each ESP consumes 4.76 mWh in each 5 minute cycle if one signal is sent to collect data. The ESPs can also run for 25.56 hours if never collecting and sending, or for 5.16 hours if always sending data (to get a sense of how much more power is consumed in order to send data to the server).

Conclusions and Lessons Learned
=========================================================================

Currently, our system heavily depends on accurate data from all devices and good signal detection. This makes it susceptible to wildly inaccurate localizations due to outliers and inaccurate detections, observable from frequent failures. However, good data and signal detection can lead to accurate and precise results.

In going through all the tests and deliberations to reach our final product, we explored several different electronics and their features. We learned that running different code can make ESP clocks drift much more quickly. We learned that it is important to demonstrate feasibility within the specs of each component. We learned what it takes to transmit signals over long distances with simple devices.

A possible modification to improve the accuracy shortcomings would involve placing multiple ESPs in the same location to better detect outliers. In addition, if only two locations were used, it would be a more simple proof of concept simply to localize sound within the line they define.

Acknowledgements
=========================================================================

Professor Max Shulaker for mentoring our team and coming straight from an international flight to work with us on a weekend.
Claire and Mark for being our TAs with our project, offering sound suggestions, and being available during office hours as well.
Professors Joe Steinmeyer and and Stefanie Mueller for organizing and teaching the class.
The rest of the course staff for helping with questions and presenting material.

Resources
====

[1] “3D sound localization” (2019). Wikipedia. Retrieved from https://en.wikipedia.org/wiki/3D_sound_localization.

[2] Irie, Robert E. (1995). “Robust sound localization: An application for an auditory perception system for a humanoid robot,” Thesis, MIT Department of Electrical Engineering and Computer Science.

<!-- Markdeep: --><style class="fallback">body{visibility:hidden;white-space:pre;font-family:monospace}</style><script src="markdeep.min.js" charset="utf-8"></script><script src="https://casual-effects.com/markdeep/latest/markdeep.min.js?" charset="utf-8"></script><script>window.alreadyProcessedMarkdeep||(document.body.style.visibility="visible")</script>
